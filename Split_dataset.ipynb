{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following file contains code for creating sub files of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from tqdm import tqdm, trange\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv('./data/final/dataset.csv', encoding=\"latin1\")\n",
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to group the documents\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "class DocumentGetter(object):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.documents = self.data.groupby(\"File_id\").apply(agg_func)\n",
    "        \n",
    "        \n",
    "        self.train_size = 0.8\n",
    "        self.test_size = 0.1\n",
    "        self.dev_size = 0.1\n",
    "    \n",
    "    # get next document\n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"{}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    # split documents\n",
    "    def split_docs(self):\n",
    "        df = shuffle(self.documents)\n",
    "        \n",
    "        train_size = round(0.8 * len(df))\n",
    "        self.tr_documents = df[:train_size]\n",
    "                           \n",
    "        test_size = round(train_size + (0.1 * len(df)))\n",
    "        self.te_documents = df[train_size:test_size]\n",
    "        \n",
    "        self.de_documents = df[test_size:len(df)]\n",
    "    \n",
    "    # convert a dict to a df\n",
    "    def dict_to_df(self, d):\n",
    "        ids = []\n",
    "        tokens = []\n",
    "        tags = []\n",
    "        for key in d:\n",
    "            doc = d[key]\n",
    "            \n",
    "            for (token, tag) in doc:\n",
    "                ids.append(key)\n",
    "                tokens.append(token)\n",
    "                tags.append(tag)\n",
    "        \n",
    "        data = {\n",
    "            'File_id': ids,\n",
    "            'Word': tokens,\n",
    "            'Tag': tags\n",
    "        }\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    # create the train dev test csv's\n",
    "    def create_files(self):\n",
    "        \n",
    "        df_tr = self.dict_to_df(dict(self.tr_documents))\n",
    "        df_te = self.dict_to_df(dict(self.te_documents))\n",
    "        df_de = self.dict_to_df(dict(self.de_documents))\n",
    "        \n",
    "        \n",
    "        print('Train', len(Counter(df_tr['Tag'].values)))\n",
    "        print('Test', len(Counter(df_te['Tag'].values)), Counter(df_te['Tag'].values))\n",
    "        print('Dev', len(Counter(df_de['Tag'].values)), Counter(df_de['Tag'].values))\n",
    "        df_tr.to_csv('./data/final/train.csv', index=False)\n",
    "        df_te.to_csv('./data/final/test.csv', index=False)\n",
    "        df_de.to_csv('./data/final/dev.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the train test and dev files\n",
    "doc_Getter = DocumentGetter(df)\n",
    "doc_Getter.split_docs()\n",
    "doc_Getter.create_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the .txt file\n",
    "def create_file(t, s_len, n_dataset):\n",
    "    f_path = './data/final/' + t + '.csv'\n",
    "    d = prepare_data(f_path, s_len)\n",
    "    getter = SentenceGetter(d)\n",
    "    sentences = [[word[0] for word in sentence] for sentence in getter.sentences]\n",
    "    labels = [[s[1] for s in sentence] for sentence in getter.sentences]\n",
    "    \n",
    "    c = list(zip(sentences, labels))\n",
    "    random.shuffle(c)\n",
    "    sentences, labels = zip(*c)\n",
    "    \n",
    "    SIZE = 1\n",
    "    \n",
    "    split = round(len(sentences)*SIZE)\n",
    "    print(len(sentences), len(labels))\n",
    "    sentences = sentences[:split]\n",
    "    labels = labels[:split]\n",
    "    \n",
    "    print(len(sentences), len(labels))\n",
    "    create_txt_file(sentences, labels, t, s_len, n_dataset)\n",
    "    \n",
    "# iniit the variables and prepare the data    \n",
    "def prepare_data(file_path, s_len):\n",
    "    # load dataframe\n",
    "    df = pd.read_csv(file_path, encoding=\"latin1\")\n",
    "    \n",
    "    # drop nan\n",
    "    df = df.dropna()\n",
    "    \n",
    "    padding_length = 4\n",
    "    sentence_length = s_len\n",
    "    splits = split_in_sentences(sentence_length, padding_length, df)\n",
    "    output = filter_splits(splits, False)\n",
    "    output['Word'] = output['Word']\n",
    "    output = output.dropna()\n",
    "    return output\n",
    "\n",
    "# filter the sentence splits\n",
    "def filter_splits(d, f=True):\n",
    "    if f:\n",
    "        split_tag = d.groupby('Split #')['Tag'].apply(list)\n",
    "        to_remove = []\n",
    "        for key, value in split_tag.items():\n",
    "            tags_in_split = list(set(value))\n",
    "            if len(tags_in_split) == 1 and tags_in_split[0] == 'O':\n",
    "                d = d[d['Split #'] != key]\n",
    "        \n",
    "    return d\n",
    "\n",
    "\n",
    "# split each document in sentences of length s\n",
    "def split_in_sentences(sen_len, pad_len, d):\n",
    "    doc_words = d.groupby('File_id')['Word'].apply(list)\n",
    "    doc_tags = d.groupby('File_id')['Tag'].apply(list)\n",
    "    \n",
    "    splits = []\n",
    "    cur_counter = 0\n",
    "    \n",
    "    for i in range(len(doc_words)):\n",
    "        cur_doc = list(doc_words)[i]\n",
    "        cur_tags = list(doc_tags)[i]\n",
    "        cur_counter += 1\n",
    "        for j in range(len(cur_doc)):\n",
    "            splits.append('split ' + str(cur_counter))\n",
    "            if ((j % sen_len) == 0 and j != 0):\n",
    "                    \n",
    "                cur_counter += 1\n",
    "                \n",
    "    d['Split #'] = splits\n",
    "    return d\n",
    "\n",
    "# function to create .txt fiile\n",
    "def create_txt_file(s, l, name, s_len, n_dataset):\n",
    "    if (len(s) != len(l)):\n",
    "        print('Not the same length')\n",
    "        return\n",
    "    \n",
    "    f = open('./data/final/s_' +str(s_len)+ '/' + n_dataset + '/' + name +'.txt',\"w+\")\n",
    "    for i in range(len(s)):\n",
    "        sentence = s[i]\n",
    "        labels = l[i]\n",
    "        for j in range(len(sentence)):\n",
    "            \n",
    "            if (is_ascii(sentence[j])):\n",
    "                f.write(sentence[j].strip() + ' ' + labels[j] +'\\n')            \n",
    "        f.write('\\n')\n",
    "    f.close()\n",
    "\n",
    "# check if token is ascii\n",
    "def is_ascii(s):\n",
    "    return all(ord(c) < 128 for c in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class for grouping the sentences in the document\n",
    "class SentenceGetter(object):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        \n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Split #\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "\n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"{}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# create txt files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SENTENCE_LENGTHS = [25, 50, 75, 100, 150, 200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,6):\n",
    "    doc_Getter = DocumentGetter(df)\n",
    "    doc_Getter.split_docs()\n",
    "    doc_Getter.create_files()\n",
    "    for sl in SENTENCE_LENGTHS:\n",
    "        create_file('train', sl, str(i))\n",
    "        create_file('test', sl, str(i))\n",
    "        create_file('dev', sl, str(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_file('train', SENTENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_file('test', SENTENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_file('dev', SENTENCE_LENGTH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
