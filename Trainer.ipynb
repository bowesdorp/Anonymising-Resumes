{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# the following file contains the script to train the NER models on the AWS servers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !unzip Archive.zip\n",
    "\n",
    "# download the dependencies\n",
    "import sys\n",
    "!{sys.executable} -m pip install allennlp==0.9.0 flair\n",
    "\n",
    "# import the dependencies\n",
    "from flair.data import Corpus\n",
    "from flair.datasets import ColumnCorpus\n",
    "from flair.models import SequenceTagger\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.embeddings import FlairEmbeddings, TransformerWordEmbeddings, StackedEmbeddings, ELMoEmbeddings\n",
    "from flair.datasets import DataLoader\n",
    "import random\n",
    "\n",
    "# the class for training the models\n",
    "class Trainer:\n",
    "    \n",
    "    # init the class\n",
    "    def __init__(self, dataset_i,  sen_size, train_size=1.0, embedding_types=['flair'], use_crf=True, hidden_size=256,\n",
    "                learning_rate=0.05, batch_size=32, max_epochs=150):\n",
    "        \n",
    "        \n",
    "        self.data_columns = {0: 'text', 1: 'ner'}\n",
    "        self.tag_type = 'ner'\n",
    "        self.data_folder = './s_' + sen_size + '/' + str(dataset_i)\n",
    "        self.embedding_types = embedding_types\n",
    "        self.use_crf = use_crf\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.train_size = train_size\n",
    "        self.batch_size = batch_size\n",
    "        self.max_epochs = max_epochs\n",
    "        self.output_folder = \"resources/models\"\n",
    "        \n",
    "        print(f'Creating corpus')\n",
    "        self.create_corpus()\n",
    "        \n",
    "        print(f'Initializing embeddings: {\" \".join(map(str, self.embedding_types))}')\n",
    "        self.set_embeddings()\n",
    "        \n",
    "        print(f'Initializing model')\n",
    "        self.set_train_model()\n",
    "        \n",
    "        print(f'Initialization done')\n",
    "    \n",
    "    # read the sentences from the file\n",
    "    def get_sentences(self, size):\n",
    "        sentences = []\n",
    "        f = open(self.data_folder + '/train.txt', 'r')\n",
    "        lines = f.readlines()\n",
    "        f.close()\n",
    "        sentence = []\n",
    "        for line in lines:\n",
    "            if line == '\\n':\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "            else:\n",
    "                sentence.append(line)\n",
    "\n",
    "        length = len(sentences)\n",
    "\n",
    "        random.shuffle(sentences)\n",
    "\n",
    "        return sentences[:round(length*size)]\n",
    "    \n",
    "    # create a new training file with size s\n",
    "    def make_file(self, sentences, size):\n",
    "        \n",
    "        f_name = 'train_' + str(int(size*100)) + '.txt'\n",
    "        path = self.data_folder +  '/' + f_name\n",
    "\n",
    "        f = open(path, 'w')\n",
    "\n",
    "        for sentence in sentences:\n",
    "            for token in sentence:\n",
    "                f.write(token)\n",
    "\n",
    "            f.write('\\n')\n",
    "\n",
    "        f.close()\n",
    "\n",
    "        return f_name\n",
    "\n",
    "    def make_train_file(self, size):\n",
    "        tr_sentences = self.get_sentences(size)\n",
    "        path_new_file = self.make_file(tr_sentences, size)\n",
    "        return path_new_file\n",
    "    \n",
    "    # create the corpus for the trainer\n",
    "    def create_corpus(self):\n",
    "        new_train_file = self.make_train_file(self.train_size)\n",
    "        \n",
    "        self.corpus: Corpus = ColumnCorpus(self.data_folder, self.data_columns,\n",
    "                              train_file=new_train_file,\n",
    "                              test_file='test.txt',\n",
    "                              dev_file='dev.txt')\n",
    "            \n",
    "        self.tag_dictionary = self.corpus.make_tag_dictionary(tag_type=self.tag_type)\n",
    "        \n",
    "    def get_tag_dictionary(self):\n",
    "        return self.tag_dictionary\n",
    "    \n",
    "    \n",
    "    def get_subset(self, subset):\n",
    "        if subset == 'train':\n",
    "            return self.corpus.train\n",
    "        \n",
    "        if subset == 'test':\n",
    "            return self.corpus.test\n",
    "        \n",
    "        if subset == 'dev':\n",
    "            return self.corpus.dev\n",
    "    \n",
    "    # set the embeddings for the trainer\n",
    "    def set_embeddings(self):\n",
    "        \n",
    "        if len(self.embedding_types) < 1:\n",
    "            print(f'No embeddings selected')\n",
    "            \n",
    "        else:\n",
    "            embeddings = []\n",
    "\n",
    "            if 'flair' in self.embedding_types:\n",
    "                embeddings.append(FlairEmbeddings('news-forward-fast'))\n",
    "                embeddings.append(FlairEmbeddings('news-backward-fast'))\n",
    "\n",
    "            if 'elmo' in self.embedding_types:\n",
    "                embeddings.append(ELMoEmbeddings())\n",
    "\n",
    "            if 'bert' in self.embedding_types:\n",
    "                embeddings.append(TransformerWordEmbeddings('bert-base-cased'))\n",
    "\n",
    "            self.embeddings = StackedEmbeddings(embeddings=embeddings)\n",
    "            \n",
    "    # set the trainer     \n",
    "    def set_train_model(self):\n",
    "        \n",
    "        self.tagger: SequenceTagger = SequenceTagger(hidden_size=self.hidden_size,\n",
    "                                        embeddings=self.embeddings,\n",
    "                                        tag_dictionary=self.tag_dictionary,\n",
    "                                        tag_type=self.tag_type,\n",
    "                                        use_crf=self.use_crf)\n",
    "            \n",
    "        self.trainer: ModelTrainer = ModelTrainer(self.tagger, self.corpus)\n",
    "    \n",
    "    # train the NER model\n",
    "    def train(self):\n",
    "        \n",
    "        self.trainer.train(\n",
    "            self.output_folder,\n",
    "            learning_rate=self.learning_rate,\n",
    "            mini_batch_size=self.batch_size,\n",
    "            max_epochs=self.max_epochs,\n",
    "        )\n",
    "              \n",
    "# parameters              \n",
    "sentence_sizes = ['50']\n",
    "train_sizes = [0.2, 0.4, 0.6, 0.8]\n",
    "data_sets = ['2', '3', '4', '5']\n",
    "e_types = ['elmo']\n",
    "              \n",
    "import os\n",
    "\n",
    "try:\n",
    "    # Create target Directory\n",
    "    os.mkdir('results')\n",
    "except FileExistsError:\n",
    "    print(\"Directory already exists\")\n",
    "    \n",
    "for sentence_size in sentence_sizes:\n",
    "    \n",
    "    try:\n",
    "        # Create target Directory\n",
    "        os.mkdir('./results/'+ sentence_size)\n",
    "    except FileExistsError:\n",
    "        print(\"Directory already exists\")\n",
    "    \n",
    "    # do multiple runs\n",
    "    for data_set in data_sets:\n",
    "        for train_size in train_sizes:\n",
    "            t = Trainer(data_set,  sentence_size, train_size, e_types, use_crf=True)\n",
    "            t.train()\n",
    "\n",
    "            columns = {0: 'text', 1: 'ner'}\n",
    "\n",
    "            data_folder = './s_' + sentence_size + '/' + data_set\n",
    "\n",
    "            tag_type = 'ner'\n",
    "\n",
    "            corpus: Corpus = ColumnCorpus(data_folder, columns,\n",
    "                                          train_file='train_' + str(int(train_size*100)) + '.txt',\n",
    "                                          test_file='test.txt',\n",
    "                                          dev_file='dev.txt')\n",
    "\n",
    "            tag_dictionary = corpus.make_tag_dictionary(tag_type=tag_type)\n",
    "\n",
    "            model = SequenceTagger.load('./resources/models/best-model.pt')\n",
    "\n",
    "\n",
    "            eval_data = DataLoader(corpus.test, batch_size=1)\n",
    "\n",
    "            result, score = model.evaluate(eval_data.dataset, out_path=f\"./results/\" + sentence_size +  \"/predictions_\" + data_set  +\".txt\")\n",
    "\n",
    "            f = open(\"./results/\" + sentence_size + '/results_' + data_set + '_' + str(int(train_size*100)) + \".txt\", \"w\")\n",
    "            f.write(result.detailed_results)\n",
    "            f.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
